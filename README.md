# 🚀 Kubernetes Part 1 – minikube & KOps Installation

This README serves as a **complete hands-on guide** for Kubernetes learning. It includes YAML manifests and command-line usage to help you practice kubernetes related topics.

## 📚 What You’ll Learn

- ✅ How to create a Production Grade Cluster using **kOps**
- ✅ How to create a Local **minikube** Cluster

---

## 🔧 Getting Started with kOps on AWS

[Click me](https://kops.sigs.k8s.io/getting_started/aws/) for Official Documentation of kOps

### 1️⃣ Install Kops for Linux

```bash
curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/
```

### 2️⃣ Install kubectl for Linux

```bash
curl -Lo kubectl https://dl.k8s.io/release/$(curl -s -L https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
```

### 3️⃣ Setup IAM user 

In order to build clusters within AWS we'll create a dedicated IAM user for kops. This user requires API credentials in order to use kops. Create the user, and credentials, using the AWS console.

The kops user will require the following IAM permissions to function properly:

```bash
AmazonEC2FullAccess
AmazonRoute53FullAccess
AmazonS3FullAccess
IAMFullAccess
AmazonVPCFullAccess
AmazonSQSFullAccess
AmazonEventBridgeFullAccess
```

### 4️⃣ Create S3 Bucket 

In order to store the state of your cluster, and the representation of your cluster, we need to create a dedicated S3 bucket for kops to use. This bucket will become the source of truth for our cluster configuration. In this guide we'll call this bucket ==**lwm-kubernetes-bucket**==, but you should add a custom prefix as bucket names need to be unique.

### 5️⃣ Prepare local environment 

You should record the SecretAccessKey and AccessKeyID in the returned JSON output, and then use them below:

```bash
# configure the aws client to use your new IAM user
aws configure           # Use your new access and secret key here
aws iam list-users      # you should see a list of all your IAM users here

# Create ssh-keys for kOps to use
ssh-keygen -t rsa -b 4096 -f ~/.ssh/kops-ssh-key
kops create secret --name <your-cluster-name> sshpublickey admin -i ~/.ssh/kops-ssh-key.pub

# Because "aws configure" doesn't export these vars for kops to use, we export them now
export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)
export NAME=myfirstcluster.k8s.local
export KOPS_STATE_STORE=s3://prefix-example-com-state-store
```

For a gossip-based cluster, make sure the name ends with ==**k8s.local.**==

### 6️⃣ kOps Commands

```bash
kops create cluster --name=${NAME} --cloud=aws --zones=ap-southeast-1a
kops edit cluster --name ${NAME}
kops update cluster --name ${NAME} --yes --admin
kops validate cluster
kops delete cluster --name ${NAME} --yes
```

---

## 🔧 Getting Started with minikube on AWS

[Click me](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download) for Official Documentation of minikube

### 1️⃣ Installation

Create a Linux EC2 VM with minimum  
- 2 CPUs or more 
- 2GB of free memory 
- 20GB of free disk space

```bash
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
```

### 2️⃣ Start your cluster

```bash
minikube start
```

### 3️⃣ Interact with your cluster

```bash
minikube kubectl -- get pod -A
```

You can also make your life easier by adding the following to your shell config

```bash
alias kubectl="minikube kubectl --"
kubectl get pod
```

---

### 📞 Contact Us
**Phone:** [+91 91500 87745](tel:+919150087745)

### 💬 Ask Your Doubts
Join our **Discord Community**  
👉 [Click here to connect](https://discord.gg/N7GBNHBdqw)

### 📺 Explore More Learning
Subscribe to our **YouTube Channel** – *Learn With Mithran*  
🎯 [Watch Now](https://www.youtube.com/@LearnWithMithran)

---

# 🚀 Kubernetes Part 2 – YAML Reference Guide

In Part 2, you will learn how to create **Namespaces, Pods, ReplicaSets, DaemonSets.** Learn about kubectl cheat codes and both imperative and declarative way of deploying kubernetes Objects


## 📚 What You’ll Learn

- ✅ Creating and using **Namespaces**
- ✅ Defining **Pods** with one or more containers
- ✅ Using **ReplicaSets** to maintain pod availability
- ✅ Deploying **DaemonSets** to run a pod on each node

---

## 🔧 How to Use

👉 You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1️⃣ Namespace: qa

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: qa
```

### 2️⃣ Pod: myfirstpod (Single Container)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myfirstpod
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
```

### 3️⃣ Pod: mysecondpod (Multi-Container: httpd + jenkins)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysecondpod
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
  - name: cont2
    image: jenkins/jenkins
    ports:
    - containerPort: 8080
```

### 4️⃣ Pod: mythirdpod (Multi-Container: httpd + nginx)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mythirdpod
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
  - name: cont2
    image: nginx
    ports:
    - containerPort: 80
```

### 5️⃣ Pod: myfirstpod in dev Namespace

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myfirstpod
  namespace: dev
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
```

### 6️⃣ ReplicaSet: lwm-replica (5 Replicas)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: lwm-replica
spec:
  # modify replicas according to your case
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd
```

### 7️⃣ DaemonSet: lwm-daemon

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: lwm-daemon
spec:
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd
```
---

### 📞 Contact Us
**Phone:** [+91 91500 87745](tel:+919150087745)

### 💬 Ask Your Doubts
Join our **Discord Community**  
👉 [Click here to connect](https://discord.gg/N7GBNHBdqw)

### 📺 Explore More Learning
Subscribe to our **YouTube Channel** – *Learn With Mithran*  
🎯 [Watch Now](https://www.youtube.com/@LearnWithMithran)

---

# 🚀 Kubernetes Part 3 – YAML Reference Guide

In **Part 3**, you will learn how to manage applications using **Deployments** and expose them to users and other services using **Kubernetes Services**.


## 📚 What You’ll Learn

- ✅ Understand what a Deployment is and how it manages Pods  
- ✅ Create Deployments with different update strategies (RollingUpdate vs Recreate)  
- ✅ Expose Pods and Deployments using various Service types:
    - 🔹 **NodePort** – for accessing Pods externally via a fixed port on the node  
    - 🔹 **ClusterIP** – for internal Pod-to-Pod communication  
    - 🔹 **LoadBalancer** – for external access using cloud provider’s load balancer
- ✅ Demonstrate how **selectors and labels** work in real-world scenarios  
- ✅ Connect Pods internally and externally using appropriate Service types  
- ✅ Expose applications like **Jenkins** via NodePort  
- ✅ Test and validate connections using port mappings

---

## 🔧 How to Use

👉 You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1️⃣ Basic Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lwm-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd
```

### 2️⃣ Deployment with Recreate Strategy

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lwm-deployment-re
spec:
  strategy:
    type: Recreate
  replicas: 4
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd

```

### 3️⃣ Exposing a Pod with NodePort Service

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webserver
  labels:
    app: httpd
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-np-service
spec:
  type: NodePort
  selector:
    app: httpd
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30001
```

### 4️⃣ Exposing Jenkins Pod with NodePort Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-np-service
spec:
  type: NodePort
  selector:
    youtube: lwm
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30002
---
apiVersion: v1
kind: Pod
metadata:
  name: jenkins-pod
  labels:
    youtube: lwm
spec:
  containers:
  - name: cont1
    image: jenkins/jenkins
    ports:
    - containerPort: 8080
```

### 5️⃣ Pod-to-Pod Connection via ClusterIP Service

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    colour: black
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    colour: pink
spec:
  containers:
  - name: cont1
    image: nginx
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-cip-service
spec:
  type: ClusterIP
  selector:
    colour: pink
  ports:
    - port: 9999
      targetPort: 80
```

### 6️⃣ Exposing Deployment with LoadBalancer Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: lb-service
spec:
  type: LoadBalancer
  selector:
    colour: blue
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lwm-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: blue
  template:
    metadata:
      labels:
        colour: blue
    spec:
      containers:
      - name: cont1
        image: httpd
```

### 7️⃣ Showcasing Selector and Labels with Deployment and Pod

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lwm-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: blue
  template:
    metadata:
      labels:
        colour: blue
    spec:
      containers:
      - name: cont1
        image: httpd
---
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    colour: blue
spec:
  containers:
  - name: cont1
    image: nginx
    ports:
    - containerPort: 80
```

---

### 📞 Contact Us
**Phone:** [+91 91500 87745](tel:+919150087745)

### 💬 Ask Your Doubts
Join our **Discord Community**  
👉 [Click here to connect](https://discord.gg/N7GBNHBdqw)

### 📺 Explore More Learning
Subscribe to our **YouTube Channel** – *Learn With Mithran*  
🎯 [Watch Now](https://www.youtube.com/@LearnWithMithran)

---

# 🚀 Kubernetes Part 4 – YAML Reference Guide

In **Part 4**, you will learn how to securely manage user access and permissions inside your EKS cluster using:

## 📚 What You’ll Learn

- ✅ `aws-auth` ConfigMap to add IAM users and roles  
- ✅ Role-Based Access Control (RBAC) for namespace-specific and cluster-wide access  
- ✅ Create `Role`, `RoleBinding`, `ClusterRole`, and `ClusterRoleBinding`  
- ✅ Map IAM users to Kubernetes RBAC groups  
- ✅ Use **IRSA (IAM Roles for Service Accounts)** to allow Pods to access AWS services like S3 securely  
- ✅ Hands-on live demo of a Pod accessing S3 using IRSA

---

## 🔧 How to Use

👉 You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1️⃣ aws-auth ConfigMap – Adding IAM Users

```yaml
apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::992382429239:role/eks-node-group-role
      username: system:node:{{EC2PrivateDNSName}}
  mapUsers: |
    - userarn: arn:aws:iam::992382429239:user/dev-user
      username: dev-user
      groups:
        - dev-group
    - userarn: arn:aws:iam::992382429239:user/readonly-user
      username: readonly-user
      groups:
        - readonly-group
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
```

### 2️⃣ Dev User Role and RoleBinding (Namespace-scoped Access)

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev-role
  namespace: dev-namespace
rules:
- apiGroups: ["", "apps", "batch"]
  resources: ["pods", "services", "deployments", "jobs"]
  verbs: ["get", "list", "create", "update", "delete", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-binding
  namespace: dev-namespace
subjects:
- kind: Group
  name: dev-group
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-role
  apiGroup: rbac.authorization.k8s.io
```

### 3️⃣ ReadOnly User ClusterRole and ClusterRoleBinding
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: readonly-role
rules:
- apiGroups: ["", "apps", "batch"]
  resources: ["pods", "services", "deployments", "jobs"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: readonly-binding
subjects:
- kind: Group
  name: readonly-group
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: readonly-role
  apiGroup: rbac.authorization.k8s.io
```

### 4️⃣ Pod Accessing S3 Using IRSA

#### 🔹 Step 1: Enable OIDC Provider for Your EKS Cluster

```bash
eksctl utils associate-iam-oidc-provider \
  --region ap-southeast-1 \
  --cluster <cluster-name> \
  --approve
```

#### 🔹 Step 2: Create IAM Role with Trust Policy for Pod
#### 🔹 Step 3: Create Kubernetes ServiceAccount

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-access
  namespace: default
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/EKS_S3_IRSA_Role
```

#### 🔹 Step 4: Deploy a Pod Using the Service Account

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: s3-reader
spec:
  serviceAccountName: s3-access
  containers:
  - name: awscli
    image: amazonlinux
    command: ["/bin/sh"]
    args: ["-c", "yum install -y aws-cli && aws s3 ls"]
```

#### 🔹 Step 5: Verify Access to S3 from Pod

```bash
2025-06-01 00:00:00 lwm-terraform-bucket
2025-06-01 00:00:00 lwm-kubernetes-bucket
```

---

### 📞 Contact Us
**Phone:** [+91 91500 87745](tel:+919150087745)

### 💬 Ask Your Doubts
Join our **Discord Community**  
👉 [Click here to connect](https://discord.gg/N7GBNHBdqw)

### 📺 Explore More Learning
Subscribe to our **YouTube Channel** – *Learn With Mithran*  
🎯 [Watch Now](https://www.youtube.com/@LearnWithMithran)

---

# 🚀 Kubernetes Part 5 – YAML Reference Guide

In **Part 5**, you will learn advanced pod scheduling techniques in Kubernetes to control how and where your Pods run within your EKS cluster.

## 📚 What You’ll Learn

 - ✅ **Node Selector** – Assign Pods to specific nodes using simple labels
 - ✅ **Node Affinity** – Fine-grained control over Pod placement using label expressions
 - ✅ **Pod Affinity & Anti-Affinity** – Co-locate or separate Pods based on labels and topology
 - ✅ **Taints and Tolerations** – Ensure only specific Pods are scheduled on tainted nodes
 - ✅ Use **requiredDuringSchedulingIgnoredDuringExecution** and understand its impact
 - ✅ Real-world examples with **multiple tolerations** (e.g., green, blue)
 - ✅ Hands-on live demo with YAML files showing each scheduling strategy in action

---

## 🛠️ Cluster Setup: Three Worker Nodes in EKS

We will create three EKS worker nodes with custom labels and taints.

### ✅ Step-by-Step

1. Launch EKS cluster with 3 managed node groups (use console)
2. After nodes are ready, label and taint the nodes using kubectl: (in video the taint and labels are added in console you can also use the below commands)

```bash
# Label node 1 as general
kubectl label node <node-name-1> node-type=general

# Label and taint node 2 as high-cpu with taint colour=green
kubectl label node <node-name-2> node-type=high-cpu
kubectl taint node <node-name-2> colour=green:NoSchedule

# Label and taint node 3 as high-memory with taint colour=blue
kubectl label node <node-name-3> node-type=high-memory
kubectl taint node <node-name-3> colour=blue:NoSchedule
```

---

## 🔧 How to Use

👉 You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1️⃣ Basic Pod on Any Node

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

### 2️⃣ Pod with Required Node Affinity (high-memory)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-intensive-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-type
            operator: In
            values:
            - high-memory
  containers:
  - name: memory-app
    image: nginx
```

### 3️⃣ Pod with Preferred Node Affinity (high-memory)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-intensive-pod
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: node-type
            operator: In
            values:
            - high-memory
  containers:
  - name: memory-app
    image: nginx
```

### 4️⃣ Pod with Toleration for Taint (colour=green:NoSchedule)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cpu-intensive-pod
spec:
  tolerations:
  - key: "colour"
    operator: "Equal"
    value: "green"
    effect: "NoSchedule"
  containers:
  - name: cpu-app
    image: nginx
```

### 5️⃣ Deployment Tolerating Green Taint

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-app-deployment
  labels:
    app: green-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: green-app
  template:
    metadata:
      labels:
        app: green-app
    spec:
      tolerations:
      - key: "colour"
        operator: "Equal"
        value: "green"
        effect: "NoSchedule"
      containers:
      - name: green-container
        image: nginx
        ports:
        - containerPort: 80
```

### 6️⃣ Deployment Tolerating Both green and blue Taints

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multicolor-app
  labels:
    app: multicolor-app
spec:
  replicas: 8
  selector:
    matchLabels:
      app: multicolor-app
  template:
    metadata:
      labels:
        app: multicolor-app
    spec:
      tolerations:
      - key: "colour"
        operator: "Equal"
        value: "green"
        effect: "NoSchedule"
      - key: "colour"
        operator: "Equal"
        value: "blue"
        effect: "NoSchedule"
      containers:
      - name: multicolor-container
        image: nginx
        ports:
        - containerPort: 80
```

### 7️⃣ Deployment with Node Affinity and Toleration

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: compute-app-deployment
  labels:
    app: compute-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: compute-app
  template:
    metadata:
      labels:
        app: compute-app
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - high-cpu
      tolerations:
      - key: "colour"
        operator: "Equal"
        value: "green"
        effect: "NoSchedule"
      containers:
      - name: compute-container
        image: nginx
        ports:
        - containerPort: 80
```

### 8️⃣ Pod with Label for Pod Affinity Targeting

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    group: bestfriendz 
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

### 9️⃣ Pod Affinity (Schedule with bestfriendz Pods)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            group: bestfriendz
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: cont1
    image: httpd
```

### 1️⃣0️⃣ Pod Anti-Affinity (Avoid bestfriendz Pods)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: enemy-pod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            group: bestfriendz
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: cont1
    image: httpd
```

---

### 📞 Contact Us
**Phone:** [+91 91500 87745](tel:+919150087745)

### 💬 Ask Your Doubts
Join our **Discord Community**  
👉 [Click here to connect](https://discord.gg/N7GBNHBdqw)

### 📺 Explore More Learning
Subscribe to our **YouTube Channel** – *Learn With Mithran*  
🎯 [Watch Now](https://www.youtube.com/@LearnWithMithran)

---

# 🚀 Kubernetes Part 6 – YAML Reference Guide

In **part 6**, you'll explore how to persist and share data between Pods using different types of volumes. You’ll also understand how to dynamically and statically provision storage in AWS with EBS and EFS.

## 📚 What You’ll Learn

- ✅ Difference between `emptyDir`, `hostPath`, `PersistentVolume` (PV) and `PersistentVolumeClaim` (PVC)  
- ✅ Use `StorageClass` for dynamic provisioning of **EBS** and **EFS**  
- ✅ Understand how `ReadWriteOnce` and `ReadWriteMany` access modes affect Pod mounting  
- ✅ Share data between containers using `emptyDir`  
- ✅ Mount host machine files into Pods using `hostPath`  
- ✅ Use **EBS** volumes with static and dynamic provisioning  
- ✅ Use **EFS** for shared, scalable, and dynamic file storage  
- ✅ Full hands-on examples with YAML

---

### 🛠️ Cluster Setup: Enable the Add-on with Pod Identity (via Console)

- 1️⃣ Go to EKS > Your Cluster > Add-ons
- 2️⃣ Click Create Add-on
- 3️⃣ Choose aws-ebs-csi-driver & aws-efs-csi-driver
- 4️⃣ Select latest version
- 5️⃣ For IAM Role, choose:
- 6️⃣ Pod Identity (recommended)
- 7️⃣ Select or create the IAM role: AmazonEKS_EBS_CSI_DriverRole, AmazonEKS_EFS_CSI_DriverRole

---

## 🔧 How to Use

👉 You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1️⃣ Sharing Data Between Containers Using `emptyDir`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-example
spec:
  containers:
    - name: writer-cont1
      image: busybox
      command: ["/bin/sh", "-c", "echo 'Hello from writer' > /data/message.txt && sleep 3600"]
      volumeMounts:
        - mountPath: /data
          name: shared-data
    - name: reader-cont2
      image: busybox
      command: ["/bin/sh", "-c", "cat /data/message.txt && sleep 3600"]
      volumeMounts:
        - mountPath: /data
          name: shared-data
  volumes:
    - name: shared-data
      emptyDir: {}
```

### 2️⃣ Accessing Host Files Using hostPath

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-etc-example
spec:
  containers:
    - name: etc-reader
      image: busybox
      command: ["/bin/sh", "-c", "ls /host/etc && sleep 3600"]
      volumeMounts:
        - mountPath: /host/etc
          name: host-etc
  volumes:
    - name: host-etc
      hostPath:
        path: /etc
        type: Directory
```

### 3️⃣ Dynamic EBS Provisioning Using StorageClass and PVC

```yaml
# StorageClass for EBS
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
---
# PVC for EBS
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
  storageClassName: ebs-sc
---
# Pod using the dynamic EBS volume
apiVersion: v1
kind: Pod
metadata:
  name: ebs-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/data"
      name: ebs-volume
  volumes:
  - name: ebs-volume
    persistentVolumeClaim:
      claimName: ebs-pvc

```

### 4️⃣ Static EBS Volume with Manual PV and PVC

```yaml
# Static PersistentVolume (replace with actual EBS Volume ID)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-ebs-pv
spec:
  capacity:
    storage: 3Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-xxxxxxxxxxxxxx     # Replace EBS Volume id
---
# PersistentVolumeClaim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: static-ebs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  resources:
    requests:
      storage: 3Gi
---
# Pod using the static EBS volume
apiVersion: v1
kind: Pod
metadata:
  name: ebs-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/data"
      name: ebs-volume
  volumes:
  - name: ebs-volume
    persistentVolumeClaim:
      claimName: static-ebs-pvc

```

### 5️⃣ Dynamic EFS Volume Provisioning with StorageClass and PVC

```yaml
# StorageClass for EFS
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-0f8431a1f00634456  # Replace with your EFS ID
  directoryPerms: "700"
  gidRangeStart: "1000"
  gidRangeEnd: "2000"
  basePath: "/dynamic_provisioning"
---
# PVC for EFS
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: efs-sc
---
# Pod using EFS volume
apiVersion: v1
kind: Pod
metadata:
  name: efs-test-pod
spec:
  containers:
    - name: app
      image: busybox
      command: [ "/bin/sh", "-c", "echo EFS volume is working > /data/hello.txt && sleep 3600" ]
      volumeMounts:
        - name: efs-volume
          mountPath: /data
  volumes:
    - name: efs-volume
      persistentVolumeClaim:
        claimName: efs-pvc
```
